

![chatflex](./assets/product.png)

# ðŸ§  ChatFlex

**A Local-first LLM Interface â€” desktop-optimized and built for power users running Ollama and beyond.**

[ðŸŒ Visit Website](https://www.chatflex.app)


## âš™ï¸ Features

- ðŸ§µ **Chain-of-Thought Prompting**
- ðŸ“„ **Markdown Export**
- ðŸ–¥ï¸ **Desktop-First UX** (macOS / Windows)
- ðŸ”Œ **Multi-AI Provider Support** â€” customize providers beyond Ollama's defaults
- ðŸ” **Fully Local Execution** â€” no backend required
- ðŸ–¼ï¸ **Image & Document Upload**
- ðŸ” **Full Conversation Search**

---

## ðŸ’¡ Why ChatFlex?

While Ollama ships with a basic UI, ChatFlex gives you the flexibility and performance needed for real-world workflows:

| Feature | ChatFlex | Ollama UI |
|--------|----------|-----------|
| Native Ollama Support | âœ… | âœ… |
| Custom AI Provider Selection | âœ… | âŒ |
| No Backend Required | âœ… | âœ… |
| Desktop-Optimized Experience | âœ… | Basic |
| High-Performance Search & Export | âœ… | âŒ |

---

> Already running models with Ollama? ChatFlex takes you one step further â€” with a cleaner UI, better performance, and full control over your AI stack.

---
