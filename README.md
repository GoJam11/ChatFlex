

![chatflex](./assets/product.png)

<div align="center">

# ChatFlex

</div>

<div align="center">

**A Local-first LLM Interface â€” desktop-optimized and built for power users running Ollama and beyond.**

</div>

<div align="center">

[ğŸŒ Visit Website](https://www.chatflex.app) | English | [ç®€ä½“ä¸­æ–‡](./README.zh-CN.md)

</div>


## âš™ï¸ Features

- ğŸ§µ **Chain-of-Thought Prompting**
- ğŸ“„ **Markdown Export**
- ğŸ–¥ï¸ **Desktop-First UX** (macOS / Windows)
- ğŸ”Œ **Multi-AI Provider Support** â€” customize providers beyond Ollama's defaults
- ğŸ” **Fully Local Execution** â€” no backend required
- ğŸ–¼ï¸ **Image & Document Upload**
- ğŸ” **Full Conversation Search**

---

## ğŸ’¡ Why ChatFlex?

While Ollama ships with a basic UI, ChatFlex gives you the flexibility and performance needed for real-world workflows:

| Feature | ChatFlex | Ollama UI |
|--------|----------|-----------|
| Native Ollama Support | âœ… | âœ… |
| Custom AI Provider Selection | âœ… | âŒ |
| No Backend Required | âœ… | âœ… |
| Desktop-Optimized Experience | âœ… | Basic |
| High-Performance Search & Export | âœ… | âŒ |

---

> Already running models with Ollama? ChatFlex takes you one step further â€” with a cleaner UI, better performance, and full control over your AI stack.

---
