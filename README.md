

![chatflex](./assets/product.png)

# 🧠 ChatFlex

**A Local-first LLM Interface — desktop-optimized and built for power users running Ollama and beyond.**

[🌐 Visit Website](https://www.chatflex.app)


## ⚙️ Features

- 🧵 **Chain-of-Thought Prompting**
- 📄 **Markdown Export**
- 🖥️ **Desktop-First UX** (macOS / Windows)
- 🔌 **Multi-AI Provider Support** — customize providers beyond Ollama's defaults
- 🔐 **Fully Local Execution** — no backend required
- 🖼️ **Image & Document Upload**
- 🔍 **Full Conversation Search**

---

## 💡 Why ChatFlex?

While Ollama ships with a basic UI, ChatFlex gives you the flexibility and performance needed for real-world workflows:

| Feature | ChatFlex | Ollama UI |
|--------|----------|-----------|
| Native Ollama Support | ✅ | ✅ |
| Custom AI Provider Selection | ✅ | ❌ |
| No Backend Required | ✅ | ✅ |
| Desktop-Optimized Experience | ✅ | Basic |
| High-Performance Search & Export | ✅ | ❌ |

---

> Already running models with Ollama? ChatFlex takes you one step further — with a cleaner UI, better performance, and full control over your AI stack.

---
